\documentclass[a4paper,11pt]{article}

% Language, encoding, and layout
\usepackage{style}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{array}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{xcolor}
\usepackage[a4paper,margin=2.5cm]{geometry}
\usepackage{setspace}
\usepackage{comment}
\onehalfspacing
\usepackage{parskip} % no indent, space between paragraphs

% Mathematics
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bbm}
\usepackage{mathrsfs}  

% Graphics
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}

% Diagrams
\usepackage{tikz}
\usetikzlibrary{positioning, shapes.geometric, arrows}

% Colors
\usepackage{xcolor}

% Tables
\usepackage{booktabs}

% \usepackage{lmodern}  % Modernere Schrift

% Farben definieren
\definecolor{mainblue}{RGB}{0,70,127}
\definecolor{lightgray}{gray}{0.85}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
  pdftitle={Heat Conduction Simulation and PINN},
  pdfauthor={Dr. Robert Haas},
  pdfsubject={Heat Conduction Numerical Study},
  pdfkeywords={Heat Equation, PINN, Euler, Crank-Nicolson, PDE},
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

% Bibliography
%\bibliographystyle{plain}  % numerischer Stil
\bibliographystyle{abbrvnat}
% \bibliographystyle{plainnat}
% \bibliography{literatur}   % Datei literatur.bib

\usepackage{hyperref} % Für klickbare Links im PDF

% Header and footer
\def\title
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Heat Conduction Simulation}
\fancyhead[R]{\thepage}

\fancyfoot[L]{\textcolor{gray}{\small Robert Haas – Technical Software and Mathematics}}
\fancyfoot[R]{\textcolor{gray}{\small Page \thepage\ of \pageref{LastPage}}}
\renewcommand{\footrulewidth}{0.4pt}

% --- Für \pageref{LastPage} ---
\usepackage{lastpage}

\def\Meter{\mathrm{m}}
\def\Millimeter{\mathrm{mm}}
\def\Newton{\mathrm{N}}
\def\Pascal{\mathrm{Pa}}
\def\NewtonVsSquareMillimeter{\frac{\mathrm{N}}{\mathrm{mm}^2}}
\def\NewtonVsMeter{\frac{\mathrm{N}}{\mathrm{m}}}

% Title
% \usepackage{tikz}
\usepackage{lipsum} % für Blindtext

% Farben
\usepackage{xcolor}
\usepackage{lmodern}
\usepackage{fancyhdr}
\usepackage{lastpage}

% --- Farben ---
\definecolor{mainblue}{RGB}{0,70,127}
\definecolor{lightgray}{gray}{0.9}
\definecolor{footergray}{gray}{0.4}

% --- Fancyhdr: Fußzeile für alle Seiten außer Titelseite ---
\fancypagestyle{main}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
  %\fancyhead[R]{\thepage}
  \fancyhead[C]{The Heat Conduction Equation and Machine Learning}
  % \fancyfoot[L]{\textcolor{footergray}{Robert Haas – Technical Software and Mathematics}}
  \fancyfoot[L]{\textcolor{footergray}{\today}}
  \fancyfoot[R]{\textcolor{footergray}{Page \thepage\ of \pageref{LastPage}}}
}

\begin{document}


% --- Titelseite ---
\begin{titlepage}
    \centering

    % Zentrierte blaue Linie oben
    \vspace*{1cm}
    {\color{mainblue} \rule{1.0\textwidth}{2pt}}\\[0.5cm]

    % Titel
    {\Huge\bfseries Heat Conduction Simulation\\[0.3em]
    and Physics Informed Neural Networks (PINN)\par}
    
    \vspace*{0.5cm}
    {\color{mainblue} \rule{1.0\textwidth}{2pt}}\\[1.5cm]
    
    \vspace{1cm}

    % Autor
    {\Large Robert Haas}\\[0.3em]
    % {\normalsize Dr. Robert Haas Technical Software and Mathematics}\\[0.5cm]

    % Grauer Infoblock
    \vspace{4cm}
    \colorbox{lightgray}{
        \parbox{0.8\textwidth}{
            \vspace{1em}
            \centering
            {\normalsize\textbf{Documentation} • \today{ }• Version 1.0}\\[0.5em]
            {\small PDF generated with \LaTeX}
            \vspace{1em}
        }
    }

    \vfill

    % Projektinfo (Fußbereich)
    {\footnotesize
    \begin{tabular}{rl}
    \textbf{Project page:} & \href{https://github.com/Haasrobertgmxnet/HeatConduction}{https://github.com/Haasrobertgmxnet/HeatConduction}\\
    \textbf{Contact:} & \href{mailto:Haasrobert@gmx.net}{Haasrobert@gmx.net} \\
    \end{tabular}
    }

    \vspace{2cm}
    {\color{mainblue} \rule{1.0\textwidth}{2pt}}  % Zentrierte Linie unten
\end{titlepage}

\pagestyle{main}

\begin{abstract}
This documentation presents numerical studies for the 2D heat conduction equation. The heat equation is solved using classical numerical methods, such as explicit Euler and implicit Crank-Nicolson, as well as Physics Informed Neural Networks (PINNs). Comparisons between these methods are provided, highlighting stability, accuracy, and computational efficiency.
\end{abstract}

\tableofcontents
\newpage

\section{The classical Heat Equation}
We now derive the heat equation from the energy balance. For further reading see \cite[pp 207]{eck2008modellierung} and \cite[pp 105]{hoffmann2014modellierung}
Heat conduction describes the transfer of thermal energy in materials and is governed by the heat equation. The mechanism of heat conduction takes places on the scale of atoms and molecules and their local motion as mechanism to transfer heat. 
We consider an open spatial domain $\Omega \subset \mathbbm{R}^d $ where the heat conduction is investigated. 
Let $R \subset \Omega $ be an arbitrary subdomain of $\Omega $ with the following assumptions:
\begin{enumerate}
\item \textbf{Regularity:} $R$ is regular, so that all following integrals and operations on $R$ are well-defined.
\item \textbf{Absence of particle motion:} $R$ does not move over time, i.e. macroscopic motion of particles inside $R$ is not allowed.
\end{enumerate}
The heat equation can be derived from energy balance as follows. The change in time of the internal energy $\mathscr{E}(R) $ is given by the heat flux crossing the boundary of $R$ and the heat sources and sinks in $R$, i.e.
\begin{equation*}
\frac{\mathrm{d}}{\mathrm{d}t} \mathscr{E} (R) = \frac{\mathrm{d}}{\mathrm{d}t} \int_R \varrho u \mathrm{d} V = \int_R \varrho f \mathrm{d} V - \int_{\partial R } \boldsymbol{q}\cdot \boldsymbol{\nu}_{\partial R} \mathrm{d} A .
\end{equation*}
The used symbols have the following meaning:
\begin{table}[h!] 
\centering
{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{ccl}
\toprule
\textbf{Symbol} & \textbf{SI Unit} & \textbf{Description}\\
\midrule
$\varrho$ & $\tfrac{\mathrm{kg}}{\mathrm{m}^d}$ & Mass density \\
$u $ & $\tfrac{\mathrm{J}}{\mathrm{kg}}$ & Specific internal energy \\
$f$ & $\tfrac{\mathrm{W}}{\mathrm{kg}}$ & Specific hat sources \\
$\boldsymbol{q}$ & $\tfrac{\mathrm{W}}{\mathrm{m}^{d-1}}$ & Heat flux per area\\
$\boldsymbol{\nu}_{\partial R}$ & 1 & Outer unit normal at the boundary of $R$\\
\bottomrule
\end{tabular}}
\caption{Physical quantities.}
\label{TablePhysicalQuantities}
\end{table}
\\
Since $R$ is regular we can interchange time derivation and integration and apply the Gauss-Green theorem on the boundary integral to obtain
\begin{equation*}
\int_R \tfrac{\mathrm{\partial}}{\partial t}  \left( \varrho u \right) \mathrm{d} V = \int_R \left( \varrho f  -  \mathrm{div}(\boldsymbol{q}) \right) \mathrm{d} V , \text{ or }
\end{equation*}
\begin{equation*}
\frac{1}{\mathrm{meas}(R)}\int_R \tfrac{\mathrm{\partial}}{\partial t}  \left( \varrho u \right) \mathrm{d} V = \frac{1}{\mathrm{meas}(R)} \int_R \left( \varrho f  -  \mathrm{div}(\boldsymbol{q}) \right) \mathrm{d} V.
\end{equation*}
Since $R$ is an arbitrary sample volume, we can drop the integration to get
\begin{equation*}
\tfrac{\mathrm{\partial}}{\partial t}  \left( \varrho u \right) = \varrho f  -  \mathrm{div}(\boldsymbol{q}) .
\end{equation*}
We now fix the following assumptions:
\begin{enumerate}
\item[(A1)] Heat conduction is the only mechanism of heat transfer. That means convection and radiation are not present.
\item[(A2)] Heat sources and sinks in the bulk are nor present.
\item[(A3)] The specific internal energy is given by $u = c_P \cdot T $, where $c_P$ is the specific heat capacity at constant pressure and $T$ is the temperature.
\item[(A4)] The heat flux per area is given by $ \boldsymbol{q} = - \kappa \nabla T $ (Fourier's law), where $\kappa $ denotes the thermal conductivity.
\item[(A5)] The mass density $\varrho $ and the specific heat capacity $c_P$ do not change over time.
\item[(A6)] The thermal conductivity $\kappa $ is a constant in space.
\end{enumerate}
These assumptions lead to the classical heat conduction equation given by
\begin{equation}
\varrho c_P \frac{\partial T}{\partial t} = \kappa \Delta T , \text{ or }
\end{equation}
\begin{equation}
\frac{\partial T}{\partial t} = \alpha \Delta T \text{ in } \Omega ,
\end{equation}
with the thermal diffusivity $\alpha = \kappa / c_P / \varrho $. The following table assigns the physical units to the quantities just introduced.
\begin{table}[h!] 
\centering
{ \renewcommand{\arraystretch}{1.4}
\begin{tabular}{cclc}
\toprule
\textbf{Symbol} & \textbf{SI Unit} & \textbf{Description}& \textbf{Typical Value}\\
\midrule
$c_p$ & $\tfrac{\mathrm{J}}{\mathrm{kg}\cdot\mathrm{K} }$ & Heat capacity at constant pressure &  $ 4181 $\\
$\kappa $ & $\tfrac{\mathrm{W}}{\mathrm{m}\cdot\mathrm{K} }$ & Heat conductivity & $ 0.6 $\\
$\alpha $ & $\tfrac{\mathrm{m}^{d-1}}{\mathrm{s} }$ &  Thermal diffusivity & $ 1.438$ e-$7 $\\
\bottomrule
\end{tabular}}
\caption{Physical quantities.}
\label{TableTypicalValues}
\end{table}
\\
\textbf{General Assumption:} From now on we assume that $\Omega $ is a  2D quadratic domain $\Omega = (0.1)\times (0,1)$. Then the classical heat equation can be written as
\begin{equation} \label{HeatEquation2D}
\frac{\partial T}{\partial t} = \alpha \left( \frac{\partial^2 T}{\partial x^2} + \frac{\partial^2 T}{\partial y^2} \right) .
\end{equation}
\section{Finite Difference Method}
The ideas of finite difference method is to approximate spatial derivatives by numerical difference quotients. Evaluating a difference quotients at a certain point $(x,y)$ requires its neighboring points $(x-\Delta x,y)$, $(x+\Delta x,y)$, $(x,y-\Delta y)$ and $(x,y+\Delta y)$. For this purpose a set of grid points $\{ (x_i, y_j) \| (i,j) \in I\} $ in $\Omega $. Since $\Omega = ( 0,1 ) \times ( 0,1 ) $ the grid points $\{ (x_i, y_j) $ are aligned with the coordinate axes.
Setting $ h= \Delta x = \Delta y $ we approximate the second order spatial derivatives by
\begin{eqnarray*}
\Delta v(x,y) & \approx & \Delta_h v(x,y) \\
 & = & \frac{1}{h^2}\left(v(x+h, y) -2 v(x,y) + v(x-h,y) \right) \\
& + & \frac{1}{h^2}\left( v(x, y+h) -2 v(x,y) + v(x,y-h) \right),
\end{eqnarray*}
and the first order time derivative by a forward difference
\begin{equation*}
\frac{\mathrm{d} w(t) )}{\mathrm{d} t } \approx \frac{w(t+\tau )}{\tau } .
\end{equation*}
With a definite $(x,y)$ grid and an initial state $T_0 (x,y) $ at time $t=0$ an approximation for the solution $T$ at time $t= 0 +\tau $ can be obtained via
\begin{equation*}
u_1 = u_0 + \alpha h\Delta_h u_0,
\end{equation*}
where $ u_0 = T_0 $ evaluated at grid points. This can be successive be done:
\begin{equation*}
u_{k+1} = F_{\mathrm{explicit}} (u_k) = u_k + \alpha h \Delta_h u_k,
\end{equation*}
what defines the explicit Euler method. In contrast the implicit method is given via
\begin{equation*}
u_{k+1} = F_{\mathrm{implicit}} (u_k, u_{k+1}) =u_k + \alpha h \Delta_h u_{k+1},
\end{equation*}
which requires to solve a system of linear equations to get $u_{k+1} $, but the numerical behavior of the implicit method is more robust.
Combining explicit and implicit method leads to the Crank-Nicolson method as arithmetic mean of both methods
\begin{equation*}
u_{k+1} = \tfrac{1}{2}\left(  F_{\mathrm{explicit}} (u_k)  + F_{\mathrm{implicit}} (u_k, u_{k+1})  \right) .
\end{equation*}
The stability of the explicit Euler method is limited by the CFL condition:
\begin{equation*}
\tfrac{\tau }{h^2 } \leq \tfrac{1}{4\alpha }, 
\end{equation*}
whereas for the Crank-Nicolson method
\begin{equation*}
\tfrac{\tau }{h^2 } \leq \tfrac{1}{2\alpha }
\end{equation*}
must be fulfilled for $L^\infty $-stability. For more information, see \cite[p 112]{Ernst2015}.

\section{Physics Informed Neural Networks (PINN)}
\emph{Classification} or \emph{approximation} are the typical applications of neural networks. A short introduction to neural networks can be found in \cite[pp. 809]{stoecker1995formeln}, \cite{rey2011neuralnetworks} and \cite{rojas1993neuronale}. Today the main applications of neural networks contain classification. On the other hand, neural networks can be used to approximate an (unknown) function. Physics informed neural networks are a special type of neural networks to solve mathematical equations of physics and engineering. Typically these equations are ordinary or partial differential equations. Whereas classical numerical solvers outperform this deep learning approach, trained PINN models can be easily reused, if, e.g. a finer spatial grid is needed or the geometry of the spatial domain varies slightly. In good cases these tasks can be done without a new training. Furthermore PINNs can be used to estimate parameters in the equation, if an approximate solution is given, e.g. by measurements. In PINNs the neural network is used as a function approximator. This approach can be found in \cite[Section 3.2]{lopez2008}. The activation functions of a neural network span a function space and a function is approximated by a linear combinations of copies of the activation functions each having different scaling and offset in its function argument. The coefficients, the scalings and offsets are simply weights of the neural network to be adjusted during training. Key ingredient of the learning procedure is the loss function which will be minimized during training. This loss function will be built up by loss terms arising from the mathematical equation to solve.
That means that three loss terms will be added together to give the (total) training loss:
\begin{enumerate}
\item {\bf Physics loss:} The loss arising from deviations of the approximate solution $\hat{u}$ from the exact solution $u$. Generally, if $F(u) = 0$ is the mathematical equation, then the physics loss is $L_{\mathrm{Physics}} (w) = |F(w)|^2 $ where $w$ can be any admissible function. In case $w=u$ the equation $F(u) = 0$ is fulfilled and $L_{\mathrm{Physics}} (u) = 0 $ is minimal. For any approximate solution $\hat{u} $ we only have $L_{\mathrm{Physics}} (\hat{u}) = 0 $. So $L_{\mathrm{Physics}} $ penalizes deviation from the exact solution $u$.
\item {\bf Initial loss:} Similarly, let $I(u) = 0$ denote the initial condition. Then the initial loss is defined as $L_{\mathrm{Initial}} (w) = |I(w)|^2 $.
\item {\bf Boundary loss:} Finally, for a boundary operator $B(u) =0 $ the boundary loss is analogously defined as $L_{\mathrm{Boundary}} (w) = |B(w)|^2 $.
\end{enumerate}
To set up the PINN training the approach of \cite{aryal2024a} is used and extended. The source code used therein can be found in \cite{aryal2024b}. In this thesis an inhomogeneous heat conduction equation in two spatial dimensions with Dirichlet or Neumann boundary conditions is solved using PINNs. Let us shortly summarize the key features:
\begin{enumerate}
\item {\bf PINN architecture:} A feed forward neural network with five hidden layers, each of them containing 50 nodes is used. The network has input layer with three input nodes corresponding to the variables $x$, $y$, and $t$. The single node of the output layer is assigned to the approximate solution $\hat{u}$. 
\item {\bf PINN Activation function:} All layers except the output layer use the hyperbolic tangent as activation function.
\item {\bf PINN Solver:} To train the PINN the Adam solver of the Pytorch package is used with default settings and a learning rate of 1e-3.
\item {\bf Sampling:} Random samples of $N \in \{ 1000,\, 8000 \} $ points, each for $x$, $y$ and $t$, which is a Monte-Carlo sampling.
\end{enumerate}
\section{Case Studies}
\subsection{Case 1: Constant initial temperature with no-flux boundary conditions}
This case is also discussed in \cite{aryal2024a}. With the introduces PINN setup, we use a thermal diffusivity $\alpha = 0.1 $ in the heat equation. Furthermore we have a Gaussian kernel as a bulk heat source, i.e.
\begin{equation*}
f(x,y,t) = S\cdot\mathrm{exp}\left(-\tfrac{(x^2+y^2)}{2r}\right),
\end{equation*}
with $r=0.1$ and heat strength $S=500.0 $. As initial condition at $t=0$ we set the temperature equal to 25 °C at every point in $\Omega $. We assume that there is now heat flux across the boundary of $\Omega $, i.e. $\tfrac{\partial u }{\partial n } = 0 $ at $\partial \Omega $. For the training a Pytorch scheduler is used to adjust the learning rate adaptively. The adjustment starts after a certain number of finished epochs. This number is the step size parameter of the scheduler. We have used 4,000 and 6,000 as step size. After passing this number of epochs the learning rate decreases from 1e-3 to 5e-4 and further, each time by halving. Training with three models on the this non-homogeneous initial-boundary problem leads to the following Adjustments and results:
\begin{table}[h!] 
\centering
{ \renewcommand{\arraystretch}{1.4}
\begin{tabular}{cclc}
\toprule
\textbf{Parameter} & \textbf{Model 1} & \textbf{Model 2}& \textbf{Model 3}\\
\midrule
Epochs & 8,000 & 10,000 & 20,000\\
Samples & 1,000 & 8,000 & 8,000 \\
Step size & 4,000 & 6,000 & 6,000 \\
Relative error & 19.03\% & 7.41 \% & 4.96 \% \\
\bottomrule
\end{tabular}}
\caption{Adjustment and Results.}
\label{AdjustmentAndResults1}
\end{table}
The relative error refers to the corresponding solution using the explicit finite-difference scheme. With more training epochs the relative error decreases.
\subsection{Case 2: Constant initial temperature with Robin boundary conditions}
In case 1 the domain $\Omega$ is constantly heated over time where the heat cannot dissipate over across the boundary $\partial \Omega $ due to the no-flux boundary condition imposed on $\partial \Omega $. This leads to permanently ascending temperatures over time without reaching an equilibrium. In real life such an isolated system with permanent constant heat support is not realistic. This can be overcome by either
\begin{itemize}
\item using another heat source which decreases heat supply over time to zero, or
\item imposing other boundary conditions which allow for heat flux over the domain boundary.
\end{itemize}
In this case the latter approach is used by imposing a Robin boundary condition, i.e.
\begin{equation*}
a\cdot(u-u_0 ) + b\frac{\partial u}{\partial \nu_{\partial \Omega}} = 0,
\end{equation*}
where $  \nu_{\partial \Omega} $ is the outer unit normal at the boundary $\partial \Omega $, $u_0 $ the ambient temperature far from the domain $\Omega $ and its boundary $\partial \Omega $, i.e. far from the thermal boundary layer.
With the choice $u_0 = 25\,^{\circ}\mathrm{C} $, $a=1/2 $ and $ b=1 $ we redo the simulation of case 1 where all other parameters are left unchanged.
\begin{table}[h!] 
\centering
{ \renewcommand{\arraystretch}{1.4}
\begin{tabular}{cclc}
\toprule
\textbf{Parameter} & \textbf{Model 1} & \textbf{Model 2}& \textbf{Model 3}\\
\midrule
Epochs & 8,000 & 10,000 & 20,000\\
Samples & 1,000 & 8,000 & 8,000 \\
Step size & 4,000 & 6,000 & 6,000 \\
Relative error & 7.86\% & 3.92 \% & 3.73 \% \\
\bottomrule
\end{tabular}}
\caption{Adjustment and Results.}
\label{AdjustmentAndResults2}
\end{table}
\section{Discussion}
\begin{enumerate}
\item
Calculating a solution of the Euler-Bernoulli equation using classical finite elements is much faster and more precise than using a PINN. Of course finite element methods have been just developed to solve linear mechanical problems and thus they are optimized for that purpose. The nonlinear and nested structure of  a neural network may be not adequate or optimal for that purpose.
\item
Estimating the elastic modulus from simulated deflection values is a rather data-driven problem and the PINN approach fits far better there. With a careful adjustment of the network architecture, the solver adjustments and the loss contributions it was possible to calculate results with very satisfactory precision in case of 99 samples.
\item
For one single sample the estimation of elastic modulus failed producing results with poor accuracy. The reason is an appropriate choice of initial weights of the PINN and can be solved using another initial set up.
\item
The very fine and careful setup of the network architecture, the solver settings and the loss balancing may be improved by varying these parameters in a systematic way. It is desirable to reduce the number of epochs without loosing precision. This probably can be achieved by another balancing of the loss contributions, a modification of the PINN architecture and better settings for the solver.
\item
The calculations have been done in Python using Pytorch. To reduce the calculation time an implementation in C++ would be more adequate. One of the most performant  C++ frameworks is OpenNN which is far faster than Tensorflow or Pytorch. Unfortunately, OpenNN does not really support the use of PINNs, especially there is no automatic differentiation and no possibility to access the loss function of the hidden layers via the OpenNN API. The other C++ Machine Learning framework, MLPack, has similar difficulties with PINN and does not support PINNs very well.
\item
A way to implement a PINN study in C++ could be the use of the Pytorch C++ API. That may be as not as fast as OpenNN or MLPack but faster than any Python implementation.
\end{enumerate}


\newpage
\begin{thebibliography}{9}

\bibitem{eck2008modellierung}
C.~Eck, H.~Garcke, and P.~Knabner,
\emph{Mathematische Modellierung}
1st. ed., Springer-Verlag Berlin Heidelberg, 2008.

\bibitem{hoffmann2014modellierung}
K.-H. Hoffmann and G. Witterstein, 
\emph{Mathematische Modellierung: Grundprinzipien in Natur- und Ingenieurwissenschaften}, 
Mathematik Kompakt, 1st ed. (corrected reprint), Birkhäuser, Basel, 2014.

\bibitem{Ernst2015}
O. Ernst,
\emph{Numerik partieller Differentialgleichungen, Part 2}, 
Lecture notes, University of Technology Chemnitz, 2015.
Link (checked at 2025-09-10): \url{https://www.tu-chemnitz.de/mathematik/numa/lehre/pde-2015/Folien/pdeTeil2.pdf}

\bibitem{stoecker1995formeln}
H.~Stöcker, 
\emph{Taschenbuch mathematischer Formeln und moderner Verfahren}, 
3rd rev. and ext. ed., Harri Deutsch Verlag, Frankfurt am Main, 1995.

\bibitem{rey2011neuralnetworks}
G.~D. Rey, and K.~F. Wender
\emph{Neuronale Netze}, 
2nd rev. and ext. ed., Verlag Hans Huber, Bern, 2011

\bibitem{rojas1993neuronale}
R.~Rojas,
\emph{Theorie der neuronalen Netze: Eine systematische Einführung},
Springer-Lehrbuch, 1st ed. (corrected reprint 1996), Springer-Verlag, Berlin, 1993.

\bibitem{lopez2008}
R. López González
\emph{Neural Networks for Variational Problems in Engineering}
PhD thesis, Technical University of Catalonia, 2008
[Online]. Available: \url{https://digital.csic.es/bitstream/10261/310317/1/Tesis_Roberto_López.pdf}

\bibitem{aryal2024a}
S. Aryal, 
\emph{Simulation of 2-dimensional heat conduction with physics-informed neural networks.}
Bachelor thesis, University of Applied Sciences Ravensburg-Weingarten, 2024

\bibitem{aryal2024b}
S. Aryal, 
\emph{Bachelor thesis: Pinns for 2d heat conduction.}
Sourcecode of \cite{aryal2024a}, Link (checked at 2025-09-10): \url{https://github.com/Samman2571/Bachelor_thesis_PINNs_2D_Heat_Conduction}

\bibitem{code}
Code samples used in this work: \url{https://github.com/Haasrobertgmxnet/HeatConduction}

\end{thebibliography}


\newpage

% \printbibliography

\end{document}
